\documentclass[10pt]{beamer}
\usetheme[background=light,block=fill,progressbar=foot]{metropolis}

%% ctex configuration. slow compiling, comment unless using Chinese.
\usepackage[UTF8]{ctex}
\ctexset{refname=References}
%%

\usepackage{graphicx}
\usepackage{caption}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic} 
\usepackage{transparent}
\newcommand{\supercite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\newcommand{\emoji}[1]{\text{\raisebox{-0.2em}{\includegraphics[height=1em]{emojis/#1.png}}}}

\begin{document}
	\title{Deep Learning Book}
	\subtitle{Chapter 7 \\ Regularization for Deep Learning}
	\author{Botian Shi \\ botianshi@bit.edu.cn}
	\date{March 7, 2017}
	
	\setbeamercovered{transparent=15}
	
	\begin{frame}[plain]
		\titlepage
	\end{frame}
	
	\begin{frame}
		You can download the \LaTeX\, source code of this file from \href{https://github.com/friskit-china/DLBookSlides}{\underline{Here}}.
	\end{frame}
	
	\begin{frame}{Generalization and Strategy}
		\begin{itemize}
			\item How to make an algorithm that will perform well not just on the training data, but also on new inputs?
			\pause
			\item Many strategies designed to reduce the test error, possibly at the expense of increased training error.
			\pause
			\item These strategies are known collectively as \textbf{regularization}.
			\pause
			\item Many regularization algorithm have been developed.
			\item Developing more effective regularization strategies is one of the major research efforts in the field.
			\pause
			\item In this chapter, we describe regularization in more detail, focusing on regularization strategies for deep models or models that may be used as building blocks to form deep models.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Generalization and Strategy}
		\begin{itemize}
			\item There are many regularization strategies.
			\begin{enumerate}
				\pause
				\item Put extra constrains on a machine learning model. (Adding restrictions on the parameter values.)
				\pause
				\item Add extra terms in the objective function that can be thought of as corresponding to a soft constraint on the parameter values.
			\end{enumerate}
			\pause
			\item If chosen carefully, these extra constraints and penalties can lead to improved performance on the test set.
			\pause
			\item Sometimes these constraints and penalties are designed to
			\begin{enumerate}
				\item \textbf{encode} specific kinds of \textbf{prior knowledge}.
				\item Express a generic preference for a simpler model class in order to promote generalization.
				\item make an under-determined problem determined. (Provide more information)
			\end{enumerate}
			\pause
			\item Other forms of regularization, known as ensemble methods, combine multiple hypotheses that explain the training data.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Generalization and Strategy}
		\begin{itemize}
			\item Principle: Treading increased bias for reduced variance.
			\pause
			\item An effective regularizer is one that makes a profitable trade, \textbf{reducing variance} significantly while not overly \textbf{increasing the bias}.
			\pause
			\item In practice, \textbf{an overly complex model family does not necessarily include the target function or the true data generating process, or even a close approximation}.
			\pause
			\item We almost never have access to the true data generating process so we can never know for sure \textbf{if the model family being estimated includes the generating process or not}.		
		\end{itemize}
	\end{frame}

	\begin{frame}{Generalization and Strategy}
		\begin{itemize}
			\item However, most applications of deep learning algorithms are to domains where the true data generating process is almost certainly outside the model family.
			\pause
			\item Deep learning algorithms are typically applied to \textbf{extremely complicated domains} such as images, audio sequences and text, for which the true generation process essentially involves \textbf{simulating the entire universe}.
			\pause
			\item To some extent, we are always trying to fit a square peg(the data generating process) into a round hole (our model family)\\ 『持方枘(ruì)而欲内圆凿』.
			\pause
			\item What this means is that controlling the complexity of the model is not a simple matter of finding the model of the \textbf{right size}, with the \textbf{right number of parameters}.
			\pause
			\item Insteamd, we might find that the best fitting model is a large model that has been regularized appropriately.
			\pause
			\item We now review several strategies for how to create such a large, deep, regularized model.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Parameter Norm Penalties}
		\begin{itemize}
			\item Regularization has been used for decades prior to the advent of deep learning.
			\pause
			\item Linear models allow simple straightforward and effective regularization strategies.
			\pause
			\item Most approaches are based on limiting the capacity of models by adding a \textbf{parameter norm penalty} $\Omega(\theta)$ to the objective function $J$:
			$$\tilde{\mathit{J}}(\bm{\theta};\bm{X},\bm{y})=\mathit{J}(\bm{\theta};\bm{X},\bm{y})+\alpha\Omega(\bm{\theta})$$
			where $\alpha\in[0,+\infty)$ weights the relative contribution of the norm penalty term.
			\pause
			\item Setting $\alpha$ to 0 results in no regularization. Larger values of $\alpha$ correspond to more regularization.
			\pause
			\item Optimize both $J$ and norm
			\pause
			\item Different $\Omega$ has different result.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Parameter Norm Penalties}
		\begin{itemize}
			\item We penalize \textbf{only the weights} of the affine transformation at each layer and leaves the biases unregularized.
			\pause
			\item We do not induce too much variance by leaving the biases unregularized.
			\pause
			\item Regularizing the bias parameters can introduce a significant amount of under-fitting.
			\pause
			\item We therefore use the vector $\bm{w}$ to indicate all of the weights that should be affected by a norm penalty, while the vector $\bm{\theta}$ denotes all of the parameters, including both $\bm{w}$ and the unregularized parameters.
			\pause
			\item Sometime we use a separate penalty with a different $\alpha$ coefficient for each layer.
			\item But it can be expensive to search for the correct value of multiple hyper-parameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{$L^2$ Parameter Regularization}
		\begin{itemize}
			\item The $L^2$ norm penalty commonly known as \emph{weight decay}.
			$$\Omega(\bm{\theta})=\frac{1}{2}\lVert w\rVert^2_2$$
			
			\item This regularization strategy drives the weights closer to the origin. (as well as \emph{ridge regression} or \emph{Tikhonov regularization})
			\pause
			\item We can gain some insight into the behavior of weight decay regularization. (assume no bias for simplification)
			$$\tilde{J}(\bm{w};\bm{X},\bm{y})=\frac{\alpha}{2}\bm{w}^T\bm{w}+J(\bm{w};\bm{X},\bm{y})$$
			$$\nabla_{\bm{w}}\tilde{J}(\bm{w};\bm{X},\bm{y})=\alpha\bm{w}+\nabla_{\bm{w}}J(\bm{w};\bm{X},\bm{y})$$
			
			\pause
			\item The update
			$$\bm{w}\leftarrow\bm{w}-\epsilon(\alpha\bm{w}+\nabla_{\bm{w}}J(\bm{w};\bm{X},\bm{y}))$$
			$$\bm{w}\leftarrow(1-\epsilon\alpha)\bm{w}-\epsilon\nabla_{\bm{w}}J(\bm{w};\bm{X},\bm{y})$$

			\pause
			\item Shrink the weight vector by a constant factor on each step.
			\item What happens over the entire course of training?
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Recall: Quadratic Approximation}
		\begin{itemize}
			\pause
			\item In mathematics, approximation theory is concerned with how functions can best be approximated with simpler functions.
			\pause
			\item \textbf{local linear approximation} and \textbf{taylor expansion}
			\begin{enumerate}
				\pause
				\item For example, when the independent variable of function $y=x^3$ changes, which is $\Delta x$, the variation of $y$ is
				$$\Delta y=(x+\Delta x)^3-x^3=3x^2\Delta x+3x(\Delta x)^2+(\Delta x)^3$$
				
				\pause
				\item When $\Delta x\rightarrow0$, omit last two terms: $\Delta y=3x^2\Delta x$
				\pause
				\item In general:
				$$\Delta y=f(x_0+\Delta x)-f(x_0)\approx f'(x_0)\times\Delta x$$
				$$\Delta y=f(x)-f(x_0)\text{,~}\Delta x=x-x_0$$
				$$f(x)-f(x_0)=f'(x_0)\times(x-x_0)$$
				$$f(x)=f(x_0)+f'(x_0)(x-x_0)$$
				
				\pause
				\item In order to improve the precision, we can use second-order approximation, which is the second-order Taylor series expansion.
				$$f(x)=f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2$$
			\end{enumerate}
		\end{itemize}
	\end{frame}

	\begin{frame}{$L^2$ Parameter Regularization}
		\begin{itemize}
			\pause
			\item Let $\bm{w}^*=\arg\min_{\bm{w}}J(\bm{w})$ (unregularized training cost)
			\pause
			\item Making a quadratic approximation to the objective function in the neighborhood of the value of the weights. (In DLBook, they used $\hat{J}(\bm{\theta})$, but here we use $\hat{J}(\bm{w})$ to explain easier)
			$$\hat{\bm{J}}(\bm{w})=\bm{J}(\bm{w}^*)+\frac{1}{2}(\bm{w}-\bm{w}^*)^T\bm{\mathbf{H}}(\bm{w}-\bm{w}^*)$$
			\item Where $\mathbf{H}$ is the Hessian matrix of $J$ with respect to $\bm{w}$ evaluated at $\bm{w}^*$.
			\pause
			\item There is no first-order term in this quadratic approximation, because $\bm{w}^*$ is defined to be a minimum, where the gradient vanishes.
			\pause
			\item The minimum of $\hat{J}$ occurs where its gradient
			$$\nabla_{\bm{w}}\hat{\bm{J}}(\bm{w})=\mathbf{H}(\bm{w}-\bm{w}^*)$$
			is equal to 0.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{$L^2$ Parameter Regularization}
		\begin{itemize}
			\item To study the effect of weight decay, we modify $\nabla_{\bm{w}}\hat{\bm{J}}(\bm{w})=\mathbf{H}(\bm{w}-\bm{w}^*)$ by adding the weight decay gradient.
			\pause
			\item We can solve for the minimum of the regularized version of $\hat{\bm{J}}$.
			\item We use the variable $\tilde{\bm{w}}$ to represent the location of the minimum.
			$$\alpha\tilde{\bm{w}}+\mathbf{H}(\tilde{\bm{w}}-\bm{w}^*)=0$$
			$$(\mathbf{H}+\alpha\mathbf{I})\tilde{\bm{w}}=\mathbf{H}\bm{w}^*$$
			$$\tilde{\bm{w}}=\frac{\mathbf{H}\bm{w}^*}{(\mathbf{H+\alpha\mathbf{I}})}$$
			
			\pause
			\item As $\alpha$ approaches 0, the regularized solution $\tilde{\bm{w}}$ approaches $\bm{w}^*$.
			\item But what happens as $\alpha$ grows?
		\end{itemize}
	\end{frame}
	
	\begin{frame}{$L^2$ Parameter Regularization}
		\begin{itemize}
			\item Because $\mathbf{H}$ is real and symmetric, we can decompose it into a diagonal matrix $\bm{\Lambda}$ and an orthonormal basis of eigenvectors, $\bm{Q}$, such that $\mathbf{H}=\bm{Q\Lambda Q}^T$.
			\pause
			\item Applying the decomposition $\tilde{\bm{w}}=(\mathbf{H+\alpha\mathbf{I}})^{-1}\mathbf{H}\bm{w}^*$
			\begin{eqnarray}
				\tilde{\bm{w}}&=&(\bm{Q\Lambda Q}^T+\alpha\bm{I})^{-1}\bm{Q\Lambda Q}^T\bm{w}^*\\
				&=&\left[\bm{Q}(\bm{\Lambda}+\alpha\bm{I})\bm{Q}^T\right]^{-1}\bm{Q\Lambda Q}^T\bm{w}^*\\
				&=&\bm{Q}(\bm{\Lambda}+\alpha\bm{I})^{-1}\bm{\Lambda Q}^T\bm{w}^*\\
				&=&\bm{Q}\frac{\bm{\Lambda}}{\bm{\Lambda}+\alpha\bm{I}}\bm{Q}^T\bm{w}^*
			\end{eqnarray}
			\pause
			\item We see that the effect of weight decay is to rescale $\bm{w}^*$ along the axes defined by the eigenvectors of $\bm{H}$.
			\pause
			\item Specifically, the component of $\bm{w}^*$ that is aligned with the $i$-th eigenvector of $\bm{H}$ is rescaled by a factor of $\frac{\lambda_i}{\lambda_i+\alpha}$
		\end{itemize}
	\end{frame}
	
	
	\begin{frame}[allowframebreaks]{References}
		\bibliography{Chap7}
		\bibliographystyle{plainnat}
	\end{frame}
\end{document}